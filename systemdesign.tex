\section{System Design and Implementation}

Current cloud management platforms make simplified assumptions about the hardware in the datacentre and its usage. Hardware is by default powerful rack or blade servers, they are virtualised and always on. Thus cloud management platforms on the market are suboptimal for certain use cases.

High Powered Computing (HPC) and Big Data applications require highly optimised and powerful hardware. In such applications, the overheads imposed by virtualisation are undesirable and for maximum efficiency the cluster should consist of the so-called bare-metal computing nodes. Furthermore other advantages of virtualisation such as multi-tenancy and scaling are not useful in bare-metal computing.

On the other end of the spectrum are very weak computers with limited computing power, memory, I/O throughput and storage. These machines can be a worthwhile addition to a cloud environment for running small low intensity tasks: They are significantly cheaper to traditional datacentre hardware costing some hundred euros per machine instead of thousands like a single rack server. They do not require much space to store, use less electricity and output less heat. Virtualisation may not be applicable for such machines either due to hardware not supporting virtualisation in the first place, or as the virtualisation overhead may consume large enough share of a machine's resources rendering it incapable to perform or at least severely restricting any other functionality besides virtualisation. With low end computers virtualisation benefits like multi-tenancy and running multiple operating systems in parallel may simply not be possible because of limited capabilities. Using these machines in a heterogeneous cluster requires treating them like a traditional bare-metal nodes, albeit not nearly as powerful.

In order to leverage on bare-metal nodes in a heterogeneous and possibly even in a hybrid cloud, the task schedulers require a view to the underlying infrastructure so that they can allocate tasks to nodes fitted to perform them. To extend the usage to befit hybrid clouds in addition to heterogeneous, the orchestrator has to be vendor agnostic too. This thesis presents prototype extensions to Cloudify's \cite{cloudify} client agents, which are used to communicate between the nodes and Cloudify Manager. Extensions are going to allow two things:

\begin{enumerate}
\item \textit{Allow the Manager to gain information about the nodes' hardware capabilities, a feature that is currently lacking from the project.}
\item \textit{Enable node discovery in the cluster.}
\end{enumerate}

Currently Cloudify does not allow adding nodes to the cluster while the cluster is operational. Especially with critical systems, failover in case of hardware failure is more difficult than with virtualised resources and the system administrators may not wish to bring the system offline to replace failed physical nodes. A discovery mechanism for new nodes in the cluster would ameliorate if not solve the problem, even if replacing faulty hardware is more often than not a manual task. Additionally discovery mechanism would allow \textit{Bring-your-own-host} kind of functionality.

 