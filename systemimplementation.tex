\section{Technical Implementation}

% Do these need references?
Discovery Service is implemented with Python programming language and Flask Web framework. They were chosen because all of the components in Cloudify are also written in Python and Flask is used primarily for providing REST APIs. Even though Discovery Service does not provide any REST APIs, Flask is used for configuration management and source code organisation. Naturally, if need arises in the future to expand Discovery Service with a REST API, the development work is streamlined because of the framework.

In addition to Python program, Discovery Service relies on Redis \cite{Redis} as an in-memory key-value storage. Redis is a completely separate process in addition to Discovery Service. The preferred way of deploying Redis is in a docker container as it doesn't require installation or configuration save for exposing a correct port in the container and specifying Redis' address to Discovery Service. Redis could also be installed in the host system or even a remote system, though latter option has no practical purpose due to network latency as Redis achieves its high performance by storing values in the memory instead of disk.

\begin{figure}[ht!]
\centering
  \includegraphics[width=12cm,height=12cm, keepaspectratio]{Discovery-service-communication.png}
  \caption{The communication relationships between Discovery Service and other components}
  \label{fig:communications}
\end{figure}

On the source code level, Discovery Service consists of two major components: The network scanner and request service. The network scanner is given a subnet as a parameter and it constantly sniffs the network detecting joining and already present devices and keeping track of them. Its other task is to periodically send health checks to known devices and if a health check fails enough times, it removes the given device from the logical host pool. Request service is responsible for sending HTTP requests to the Cloudify Host-pool service. It is called by the Request Service and it runs asynchronously. In addition to HTTP requests, it performs checks to ensure that the state of Discovery service's and Host-pool service's databases correlate.

The communication relationships between different components in the system are depicted in figure \ref{fig:communications}. The source code for Discovery Service as well as the documentation can be found at \url{https://bitbucket.org/Fleuri/discoveryserviceforcloudify/src/master/}

\subsection{Network Scanner}

The Network Scanner is responsible of monitoring the network allocated for the Cloudify-orchestrated bare-metal cluster. It does so by passively listening to the network traffic but also by actively pinging the already discovered hosts. Network Scanner has two main functions, sniffer and pinger, and they run concurrently on two threads. Sniffer listens to ARP packets in the network and upon receiving one, stores the details of the sender device. Pinger function periodically sends ARP pings to previously discovered device and keeps track of their successes, eventually removing unresponsive devices from the logical host pool. In addition to two main functions there's a start-up function that initialises both local Redis storage and the Host-pool service's database. It pings all of the IP addresses in the given IP range and stores the found device details to databases.

\subsubsection{Sniffer}

Sniffer is the part of the Network Scanner used to passively listen to the network traffic in the cluster's network, and detect and store joining hosts. 

Sniffer is started in its own thread during the start up sequence of the Discovery Service. It uses \textit{Scapy} library \cite{scapy} for Python. Sniffer function is given three arguments: 

\begin{enumerate}
\item \textbf{The network interface} which the Sniffer listens to for incoming packets.
\item \textbf{The callback function} which details further instructions to perform when a packet is caught.
\item \textbf{The filter} which restricts the type of packets caught.
\end{enumerate}

Only the interface can be set by the user of the Discovery Service. The callback function is the core application logic of the sniffer and the Discovery Service itself relies on sniffing ARP packets and therefore the filter is set accordingly.

When it comes to programming logic, the callback function is the most interesting part of the sniffer. Its purpose is to evaluate whether an ARP request comes from a new or know device and store details about them. When the function receives an ARP packet it first filters out packets that are not standard ARP requests. There are two such cases: ARP Probe \cite{rfc5227}, in which the source IP address or hardware address of the ARP request is \textit{0.0.0.0} or \textit{00:00:00:00:00:00} respectively, and gratuitous ARP in which the hardware address is \textit{ff:ff:ff:ff:ff:ff}. The user can also define a list of IP and hardware addresses which the Discovery Service should ignore i.e. devices on which Cloudify should not run workloads. Such devices include the host on which Cloudify manager runs and network devices such as routers.

Next the function checks whether the packet's origin is an already known host by querying Redis. If the host is not previously known or if it is a known device with a changed IP address, the function starts a new thread to add or patch the host to the Host-pool service. See section \ref{requestservice}.

Whether the packet's origin host is known or not, the next step in the function is to insert values extracted from the packet to Redis. The data structure Discovery service uses is simple, being a hash table with the devices' hardware address being a key and value being a dictionary object consisting of the given device's IP address and the number of failed ping attempts. See section \ref{pinger} for more details. If the packet's origin is a new host, its key and values are inserted to the data store with number of failed pings always being zero. If a device is already known, its hardware address is already stored to Redis and therefore its values are modified: in most cases only the failed ping count is reset to zero but there could be cases in which the device's IP address has changed and it is updated here accordingly. Algorithm \ref{fig:snifferalgo} details the structure of the function. Note that both adding a new host and patching an already known host is done in the same Request Service function. See section ~\ref{addtohostpool} for details.

\begin{center}
\begin{algorithm}[H]
\label{fig:snifferalgo}
\begin{center}
\end{center}
\KwIn{Packet}
\If{Packet is an ARP packet}{
	\If{Packet is not an ARP Probe or Gratuitous ARP}{
	 	\If{Hardware address not found in Redis or IP address not found in Redis}{
	 	RequestService.Register\_a\_new\_host()\;
	 	}
		Redis.store(Packet.HardwareAddress: \{ip\_address: Packet.IPAddress, ping\_timeouts: 0\})\;
	}
}
\caption{Sniffer Callback Function}
\end{algorithm}
\end{center}

\subsubsection{Start-up routine} \label{startup}

Related to the sniffer function, when initializing the Discovery service, a start-up routine is run. It has three functions:

\begin{itemize}
\item It flushes the Redis key-value store
\item It empties the logical host-pool on the Host-pool Service.
\item It scans every IP address in the cluster network, adding any device found to the logical host-pool.
\end{itemize}

The routine follows the steps outlined above. First a flushing call is made to Redis. Then, using the Request Service detailed in the section ~\ref{requestservice}, the start-up routine retrieves the ids of the current hosts in the host-pool service delete the entries one by one.

Finally the routine sends an ARP ping to each hosts (Manually excluded hosts do not apply) and upon receiving a reply, stores the details of the host as described in the section ~\ref{fig:snifferalgo}. At the prototypical state, the network scan waits for every host to either reply or timeout, making the start-up routine slow if subnet's IP range is large. Parallelism and other possible future optimisations are discussed in section ~\ref{future}

\subsubsection{Pinger} \label{pinger}

Pinger is the part of the Network scanner which is used to perform health checks on existing hosts in the network and subsequently remove them from storage were they to fail them a certain number of times.

Pinger function is started in its own thread in the Network Scanner. Its responsibility is to keep track of the health of the nodes in the network. If Pinger discovers an unreachable node, it is removed from both Redis' and Host-pool service's storage.

Pinger periodically works through a list of known hosts in the network sending an ARP ping to each host. Upon receiving a response it resets the corresponding host's ping time-out counter to zero. If Pinger does not receive a response, it increments the given hosts' ping time-out by one. If after this operation the time-out crosses the given ping time-out threshold, the host is assumed to have disconnected from the network and is removed from the storage. After Pinger has pinged every host in the network, the process waits for a given ping interval after which it restarts the process.

The user gives inputs two parameters in a configuration file for Pinger to use: \textit{ping\_timeout\_threshold} and \textit{ping\_interval}. Ping\_timeout\_threshold specifies the maximum consecutive ping failures that can occur before a host is marked unreachable and removed from storage. Ping\_interval is the duration in seconds of which Pinger waits after each round of pinging the network. The more detailed presentation of the function is presented in the algorithm ~\ref{fig:pingeralgo}.

\begin{center}
\begin{algorithm}[H]
\label{fig:pingeralgo}
\begin{center}
\end{center}

\While{True}{
	\ForEach{host in Redis}{
		timeouts = host.ping\_timeouts\;
		response = Ping(host)\;
	 	\If{response}{
	 		Redis.patch(host, \{ping\_timeouts: 0\})\;
	 	}
	 	\Else{
	 		timeouts++\;
	 		\If{timeouts >= ping\_timeout\_threshold}{
	 			RequestService.delete(host)\;
	 			Redis.delete(host)\;
		} \Else {
			Redis.patch(host, \{ping\_timeouts: timeouts\}\;		
		}		
	}
}
Sleep(ping\_interval)\;
}

\caption{Pinger Algorithm}

\end{algorithm}
\end{center}

\subsection{Request Service} \label{requestservice}

Request Service is responsible of communication between the Discovery Service and Host-pool Service. As seen on figure ~\ref{fig:communications}, it is called by Network Scanner and it makes requests to Host-pool Service. The REST API Host-pool Service provides is quite succinct but provides a typical CRUD interface for handling nodes in the network. The methods are as follows\footnote{\textit{Note: Paths are relative to Host-pool service base URL e.g. localhost:8081/hosts}} 

\begin{description}

\item[\textbf{[GET] /hosts}] \hfill \\
GET request to /hosts returns a JSON list of hosts and their details. Also accepts certain filters.

\item[\textbf{[POST] /hosts}] \hfill \\
POST request to /hosts with a valid JSON array will add one or more hosts to Host-Pool service's storage See listing ~\ref{lst:JSONformat} for the JSON schema definition. Returns Idâ€™s of the new host or hosts.

\item[\textbf{[GET] /host/{id}}] \hfill \\
Returns details of a single host corresponding to the ID number.

\item[\textbf{[PATCH] /host/{id}}] \hfill \\
PATH request allows updating specified fields of a host with the given ID.

\item[\textbf{[DELETE] /host/{id}}] \hfill \\
Removes the host with given ID from Host-pool service's storage.

\item[\textbf{[POST] /host/allocate}] \hfill \\
This API call allocates a host to be used by the Cloudify Manager.

\item[\textbf{[DELETE] /host/{id}/deallocate}] \hfill \\
This returns a previously allocated host back to the host pool to be reallocated again.

\end{description}

Request Service interacts with all of the REST API endpoints except for allocation and deallocation endpoints which are interacted by Cloudify Manager's Host-pool plugin.

On source code level a typical Request Service function either makes an HTTP request to a certain endpoint, possibly with an ID corresponding to a host in the network, or constructs a JSON payload and sends that along with a POST request. Programmatic challenge in Request Service rises from the fact that the data model Host-pool service accepts is significantly more robust than that of the Discovery service, as can be seen on listing ~\ref{lst:JSONformat}. If Discovery Service's data model as seen on listing ~\ref{lst:discoformat} was a subset of Host-pool service's, handling data in Request Service would be trivial. However as Discovery Service's data model uses hardware addresses as unique identifiers whereas Host-Pool Service attaches a running number to each host. Hardware addresses are used as they are immutable in the cluster use-case and Discovery Service accounts for possibly changing IP addresses. However, Hardware addresses are by to Host-pool Service implicitly, as Discovery Service assigns each host's hardware address as a value for 'name' key. \pagebreak

\begin{lstlisting}[language=json,firstnumber=1, caption={JSON schema accepted by the Host-pool Service for a single host}, captionpos=b, label=lst:JSONformat]
hosts: {
	id: Integer
	name: String
	os: String
	endpoint {
		ip : String
		protocol: String
		port: Integer	
	}
	credentials: {
		username: String
		password: String
		key: String
	}
	tags: Array
	allocated: Boolean
	alive: Boolean
}
NOTES:
- name is an arbitrary string, but Discovery Service assigns the host's hardware address as the value of name.
- os can be either 'linux' or 'windows'. Other values are invalid.
- endpoint.ip has to be a valid IP address range with CIDR notation. If a range is defined, Host-pool Service considers each unique IP address a single host.
- protocol can be either 'winrm-http', "'winrm-https' or 'ssh', but Host-pool service does not explicitly force this.
- Host-pool Service manages id, allocated and alive fields. For User, all other fields except credentials.password, credentials.key and tags are obligatory.
- In addition to 'hosts' key, Host-pool Service also accepts 'defaults' key. 'defaults' can contain the same keys as 'hosts'. If 'defaults' is provided, its values are appended to each host which has respective undefined values. id, allocated and alive cannot be provided as defaults.

\end{lstlisting}

\begin{lstlisting}[language=json,firstnumber=1, caption={Discovery Service data format for a single host}, captionpos=b, label=lst:discoformat]
hwaddress: {ip_address: "string", ping_timeouts:  "integer"}
\end{lstlisting}

\subsubsection{Id checking}

Due to differing identifiers, certain functions are implemented to keep the two data stores synchronized. A \verb|get_id()| -function retrieves a JSON object of hosts as depicted in algorithm ~\ref{fig:get_id}. Then it compares stored IP addresses for each host or until a match is found after which the ID is returned. If hardware address is passed as an argument, the function also compares it to \emph{name} -field's value, as Discovery Service names hosts in Host-pool Service after their hardware addresses. Hardware address check is used when Network Scanner finds an already known host with a changed IP address and calls Request Service to patch it to Host-pool Service. The ID itself is used for REST API calls that are directed at a single host.

\begin{center}
\begin{algorithm}[H]
\label{fig:get_id}
\begin{center}
\end{center}
\KwIn{ip\_address}
\KwIn{hardware\_address = None}
\ForEach{host in RequestService.get\_hosts()}{
	\If{host.endpoint.ip is ip\_address or
		name is not None and 
		host.name is hardware\_address}{
	 		\Return{host.id}\;
	 	}
}
\Return{None}\;
\caption{get\_id -function compares stored IP addresses and optionally hardware addresses to find out the corresponding ID number in Host-pool Service.}
\end{algorithm}
\end{center}

\subsubsection{Adding a new host to Host-pool Service} \label{addtohostpool}

When Network Scanner detects a new host, it adds it to Redis key-value storage and starts a Request Service routine \verb|register_a_new_host| depicted in algorithm ~\ref{fig:addhost} in
a new thread to add the details also to Host-pool service. If a new IP address is detected but with an existing hardware address, the same routine is called. The \verb|register_a_new_host| function takes existing hosts into account and branches to patching function if need be. However, IP address changing for a host is a rare occasion and thus the function branching logic is done in Request Service for optimisation and maintaining source code readability.

\begin{center}
\begin{algorithm}[H]
\label{fig:addhost}
\begin{center}
\end{center}
\KwIn{ip\_address}
\KwIn{hardware\_address = None}
{
stored\_id = get\_id(ip\_address, hardware\_address)
\If{stored\_id is None} {
	data = *formatted json object corresponding to host's details*\;
	response = *POST request to Host-pool Service with 'data' as payload.*\;
	\Return{response.json, message}
} \Else {
	\Return{RequestService.patch\_a\_host(stored\_id, ip\_address, hardware\_address)}
}
}

\caption{get\_id -function compares stored IP addresses and optionally hardware addresses to find out the corresponding Id number in Host-pool Service.}
\end{algorithm}
\end{center}

\subsubsection{Limitations and assumptions of the Discovery Service}

At its current prototypical state Discovery Service makes certain assumptions about the physical hosts in the cluster.
Namely hosts are assumed to have Linux as an operating system (Distribution can vary) and a default user name and a password, those being 'centos' and 'admin' respectively. In addition hosts are required to be running an SSH server which is a requirement enforced to Linux hosts by Cloudify itself. Discovery Service requires that the standard port 22 for SSH is open.

\subsubsection{Patching a host}

Host-pool Service allows duplicate hosts, a feature which can be regarded as an oversight by the Host-pool Service developers. Discovery Service enforces that single host has only a single recorded IP address stored in the system. Therefore, if Discovery Service detects a host which has an already existing IP or Hardware Address, but the other one differs from the already stored one, Discovery Service will patch the given address with the new one. The patching to Redis is done in the Network Scanner as previously seen in algorithm~\ref{fig:snifferalgo}. The patching to Host-pool Service is depicted in algorithm~\ref{fig:patch}

\begin{center}
\begin{algorithm}[H]
\label{fig:patch}
\setlength{\AlCapSkip}{1em}
\begin{center}
\end{center}
\KwIn{id}
\KwIn{ip\_address}
\KwIn{hardware\_address}

data = \{\}\;
host = RequestService.get\_a\_host(id)\;

\If{host.ip != ip\_address} {
	data['ip'] = ip\_address\;
} 
\ElseIf{host.name != hardware\_address}{
	data['name'] = hardware\_address\; 
}
\Else { \Return{ \{\}, message}}

response = *PATCH request to Host-pool Service with 'data' as payload.*\;
\Return {response.json, message}

\caption{patch\_a\_host function is called by register\_a\_new\_host when Discovery Service detacts a host whose IP address has changed or a new host which uses the same IP address as another host previously.}

\end{algorithm}
\end{center}

The function \verb|patch_a_host| takes three arguments: The Id of the host in Host-pool Service, host's IP address and its hardware. Then it retrieves the host's details from Host-pool Service in JSON format and compares the IP addresses received as an argument and retrieved from Host-pool Service. If they do not match, the argument IP address is patched as the new value to Host-pool Service using the REST function. In a rare case in which IP addresses match, but hardware addresses do not, the hardware address is patched to Host-pool Service as a name for the host. This is a highly unlikely event and in most cases happens because there are pre-configured hosts in the Host-pool Service and the database is not formatted before deploying Host-Pool Service causing the hosts to have names that do not conform to format enforced by Discovery Service i.e. names are hardware addresses.

\subsection{Specification retriever}

As seen on listing ~\ref{lst:JSONformat}, Host-pool Service's data format has a list object named 'tags' which can hold an arbitrary number of arbitrary keywords used to describe a given host. Typical uses for these tags would be to describe a Linux distribution a host runs as the 'os' object only accepts 'linux' or 'windows' or vaguely describe the physical capabilities of a host, for example 'small' or 'large'. Host-pool plugin can use these tags to filter applicable hosts when they are allocated to Cloudify manager to use. The other attribute which can be used for filtering is the 'os' key. The problem with the tags however is the fact that they like many other attributes in Host-pool service have to be applied manually.

As one of my goals is to allow Cloudify Manager to make more informed decisions based on hosts' hardware capabilities and Host-pool plugin already having a filtering capabilities, it should be straightforward to extend that capability to also include hardware data. That however is not in the scope of this project. To leverage on hardware data, a way should exist to acquire it easily and automatically. That is why I have extended the Host-pool Service to contact the hosts added to the logical host-pool by the Discovery service and retrieve their individual hardware specifications. The source code for the custom Host-pool Service can be found here: \url{https://github.com/Fleuri/cloudify-host-pool-service}.

\subsubsection{Technical implementation of the Specification Retriever}

Specification retriever should retrieve actual hardware data from the target hosts, it should work without changing other functionality or user experience on Cloudify Manager, Host-pool Plugin, Host-pool Service or Discovery Service and it should work automatically without requiring user input or configuration.

These design goals in mind I have extended the Host-pool Service so that when a host is added to the logical host-pool, Host-pool Service runs a series of scripts on the given host which returns hardware data such as amount of RAM, number of CPUs and available disk space on that host.
It also adds the received data to Host-pool Service's data format so that it can be queried and patched using Host-pool Service's REST API. The scripts can be run because each host's data record includes their IP address, port and remote access protocol as well as access credentials. Due to prototypical limitations, the Specification Retriever presented in this thesis only works on Linux hosts as it uses Linux commands in the scripts.

\pagebreak

\lstinputlisting[language=python, firstnumber=417, firstline=417, lastline=443, numbers=left,breaklines=true, showstringspaces=false, caption="Hardware spec retriever. This caption needs improvement", captionpos=b, frame=leftline, label=lst:specretriever]{/home/fleuri/Projects/Cloudify/cloudify-host-pool-service/cloudify_hostpool/rest/backend.py}

Specification Retriever is two additional functions implemented in Host-pool Service seen on listing ~\ref{lst:specretriever}. They are called the backend class' \verb|add_host| seen on listing ~\ref{lst:hpsaddhost} which in turn is called whenever a new host is discovered in the network. Before Specification Retriever is run, the \verb|HostAlchemist.parse()| function on line 283 checks the validity of the received host data and makes a couple of additions resulting in a data element similar to one depicted in listing ~\ref{lst:JSONformat}. This element, \textit{hosts} as seen on listing ~\ref{lst:JSONSpecs}, is then passed to Specification retriever in which the hardware data is added to it.

\lstinputlisting[language=python, firstnumber=277, firstline=277, lastline=285, numbers=left,breaklines=true, showstringspaces=false, caption=Host-pool Service's backend's add\_host function makes a call to Specification Retriever, captionpos=b, frame=leftline, label=lst:hpsaddhost]{/home/fleuri/Projects/Cloudify/cloudify-host-pool-service/cloudify_hostpool/rest/backend.py}

Specification Retriever's \verb|retrieve_hardware_specs| function takes the \textit{hosts} object as a parameter as it makes modifications to it and uses its data to perform the remote operations on host machines. After checking if the added host is a Linux host (In Discovery Service's case the OS is forced) the function establishes an SSH connection using the \textit{Paramiko} library\cite{paramiko}. The arguments required to establish the connection, namely the host's IP address, port, user name and password are extracted from \textit{hosts}. Next on line 430 there is a declaration for a key-value list consisting of keys that are to be added to \textit{hosts} and matching Linux commands to extract the value for that key in the host system. Currently Specification Retriever uses the following commands:

\begin{itemize}
\item[\textbf{lscpu}] to retrieve the number of CPU's in the host system.
\item[\textbf{free}] to acquire the amount of RAM the host system has.
\item[\textbf{df}] to list the overall disk space available in the system.
\end{itemize}

The Specification Retriever is designed so that adding new commands only requires listing them to \verb|command_list| along with the identifying key. 

After connecting to the host, the \verb|run_command| routine on line 417 is ran for each command in the \verb|command_list|. It runs the command on the host and reads the result from the host's standard output stream storing it into the \verb|spec_array| hash table of results. After the for-loop \verb|spec_array| is finally concatenated as a value for the key \verb|hardware_specs| which is returned from the function after the connection is closed. The final list is added to the original data structure on line 284 seen in listing \ref{lst:hpsaddhost}. The resulting data structure can be seen on listing ~\ref{lst:FinalJSON} and it integrates seamlessly to existing data and functionality.

\begin{lstlisting}[language=json,firstnumber=1, caption={An example of additional data inserted by the Specification Retriever}, captionpos=b, label=lst:JSONSpecs]
"hardware_specs": {
	"ram": "3219", 
	"disk_size": "456G", 
	"cpus": "2"
	}
\end{lstlisting}

\begin{lstlisting}[language=json, firstnumber=1, caption={An example of final data structure after the Specification Retriever has inserted hardware data}, captionpos=b, label=lst:FinalJSON]
{
	"endpoint": {
		"ip": "192.168.150.2", 
		"protocol": "ssh", 
		"port": 22
		},
	"name": "08:9e:01:db:af:61", 
	"tags": [], 
	"alive": false, 
	"hardware_specs": {
		"ram": "3372", 
		"disk_size": "455G", 
		"cpus": "2"
		}, 
	"credentials": {
		"username": "centos", 
		"password": "admin"
		}, 
	"allocated": true, 
	"os": "linux", 
	"id": 1
}

\end{lstlisting}